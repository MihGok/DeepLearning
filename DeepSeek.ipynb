{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4fb5b-29fe-4a7b-959a-4fe30d3f14d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_folder_and_files(folder_path):\n",
    "    # Проверяем, существует ли указанный путь\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Путь {folder_path} не существует.\")\n",
    "        return\n",
    "\n",
    "    # Удаляем все файлы внутри папки\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.remove(file_path) # Используем os.remove для удаления файлов\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path) # Используем shutil.rmtree для удаления папок\n",
    "        except Exception as e:\n",
    "            print(f'Ошибка при удалении {file_path}. Причина: {e}')\n",
    "\n",
    "    # Проверяем, остались ли какие-либо файлы или папки внутри\n",
    "    if not os.listdir(folder_path):\n",
    "        # Если папка пуста, удаляем её\n",
    "        os.rmdir(folder_path)\n",
    "        print(f\"Папка {folder_path} успешно удалена.\")\n",
    "    else:\n",
    "        print(f\"Папка {folder_path} не пуста, удаление невозможно.\")\n",
    "\n",
    "# Пример использования функции\n",
    "folder_path = ['/home/jupyter/datasphere/project/model.ckpt','/home/jupyter/datasphere/project/DeepSeek_1,3b','/home/jupyter/datasphere/project/models','/home/jupyter/datasphere/project/checkpoint_best_1','/home/jupyter/datasphere/project/tb_logs','/home/jupyter/datasphere/project/lightning_logs','/home/jupyter/datasphere/project/datasetscache','/home/jupyter/datasphere/project/modelcache']\n",
    "for elem in folder_path:\n",
    "    delete_folder_and_files(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a0b99-af6d-4032-8780-6934cd2932c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install comet-ml\n",
    "import comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85e15b-f1fb-4e17-9e75-be81fbca95f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363387a-4c33-49b6-9150-2dac1bedc246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade awscli\n",
    "%pip install --upgrade boto3\n",
    "%pip install --upgrade git+https://github.com/dask/s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e3605-58a7-45b0-a8a0-3a7d3ac6081e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import  DatasetDict,load_dataset\n",
    "train_dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"php\", split=\"train\")\n",
    "test_dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"php\", split=\"test[:3000]\")\n",
    "validation_dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"php\", split=\"validation[:2000]\")\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb01d9-8ce7-4233-8bf9-b24a9173a1c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
    "max_input_length = 115\n",
    "max_target_length = 115\n",
    "prefix = 'Summerize this PHP code:'\n",
    "def preprocess_examples(examples):\n",
    "  # encode the code-docstring pairs\n",
    "    codes = examples['code']\n",
    "    docstrings = examples['docstring']\n",
    "    inputs = [prefix + code for code in codes]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "  # encode the summaries\n",
    "    labels = tokenizer(docstrings, max_length=max_target_length, padding=\"max_length\", truncation=True).input_ids\n",
    "    labels_with_ignore_index = []\n",
    "    for labels_example in labels:\n",
    "        labels_example = [label if label != 0 and label!=32014 and label!=185 and label!=31 and label!=13 else -100 for label in labels_example]\n",
    "        labels_with_ignore_index.append(labels_example) \n",
    "    model_inputs[\"labels\"] = labels_with_ignore_index\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fa534-d50f-4683-8d7f-acc847e16789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(preprocess_examples, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba249f2-902b-4dc6-90ce-bdccb39cf0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "train_dataloader = DataLoader(dataset['train'],  batch_size=1)\n",
    "valid_dataloader = DataLoader(dataset['validation'], batch_size=1)\n",
    "test_dataloader = DataLoader(dataset['test'], batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d22f3-aa5d-4463-94e3-b31e07e713ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e9af7-b63a-44ff-82dd-377d81f5e507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AdamW, get_linear_schedule_with_warmup\n",
    "import sacrebleu\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import nltk\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6b744-3562-43e1-8720-bfe4cfd125a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176779fb-7d72-4041-82c0-b39f7a43bb15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import nltk\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from nltk.translate.bleu_score import corpus_bleu,SmoothingFunction\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "class DeepSeek(pl.LightningModule):\n",
    "    def __init__(self, momentum = 0.88, lr=8e-5, weight_decay = 1e-1, num_train_epochs=50, warmup_steps=1000,max_lr=2e-4):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        return loss, outputs.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        start_time = torch.cuda.Event(enable_timing=True)\n",
    "        end_time = torch.cuda.Event(enable_timing=True)\n",
    "        start_time.record()\n",
    "        loss, logits = self.common_step(batch, batch_idx)\n",
    "        labels = batch[\"labels\"]\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        self.log(\"training_loss\", loss, on_epoch=True)\n",
    "        bleu = self.calculate_bleu(labels, preds)\n",
    "        self.log(\"training_bleu\", bleu, on_epoch=True)\n",
    "        end_time.record()\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_time = start_time.elapsed_time(end_time)\n",
    "        self.log(\"testing_time_per_batch\", elapsed_time)\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(self,batch,batch_idx):\n",
    "        loss, logits = self.common_step(batch, batch_idx)\n",
    "        labels = batch[\"labels\"]\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        print(self.ids_to_text(preds) ,labels)\n",
    "        print(self.calculate_bleu(labels, preds))\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        start_time = torch.cuda.Event(enable_timing=True)\n",
    "        end_time = torch.cuda.Event(enable_timing=True)\n",
    "        start_time.record()\n",
    "        loss, logits = self.common_step(batch, batch_idx)\n",
    "        labels = batch[\"labels\"]\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "        bleu = self.calculate_bleu(labels, preds)\n",
    "        end_time.record()\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_time = start_time.elapsed_time(end_time)\n",
    "        self.log(\"testing_time_per_batch\", elapsed_time)\n",
    "        self.log(\"validation_bleu\", bleu, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        start_time = torch.cuda.Event(enable_timing=True)\n",
    "        end_time = torch.cuda.Event(enable_timing=True)\n",
    "        start_time.record()\n",
    "        loss, logits = self.common_step(batch, batch_idx)\n",
    "        labels = batch[\"labels\"]\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        self.log(\"testing_loss\", loss)\n",
    "        bleu = self.calculate_bleu(labels, preds)\n",
    "        self.log(\"testing_bleu\", bleu)\n",
    "        end_time.record()\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_time = start_time.elapsed_time(end_time)\n",
    "        self.log(\"testing_time_per_batch\", elapsed_time)\n",
    "        return loss\n",
    "    \n",
    "    def bleu_score_metric(epoch, step):\n",
    "        return self.trainer.callback_metrics.get('validation_bleu', 0.0)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return valid_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader\n",
    "\n",
    "    def calculate_bleu(self, targets, predictions):\n",
    "        bleu_scores = []\n",
    "        smoother = SmoothingFunction()\n",
    "        for target, prediction in zip(targets, predictions):\n",
    "            reference = target.cpu().numpy().tolist()\n",
    "            hypothesis = prediction.cpu().numpy().tolist()\n",
    "            bleu_score = corpus_bleu([[reference]], [hypothesis], smoothing_function=smoother.method5)\n",
    "            bleu_scores.append(bleu_score)\n",
    "        return sum(bleu_scores)/len(bleu_scores)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Определите группы параметров для модели\n",
    "            optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            betas = (0.91,0.9999),\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "            lr_scheduler = CosineAnnealingLR(optimizer, T_max=5, eta_min = 1e-5, last_epoch=-1, verbose=True)\n",
    "            return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"monitor\": \"validation_bleu\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return valid_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b7f6a-e7af-4022-a063-add53fe2c56d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DeepSeek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608b316-7454-4899-b21a-b7f7ec604171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  56%|█████▋    | 136216/241241 [5:22:30<4:08:39,  7.04it/s, v_num=2828]"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "import torch\n",
    "from pytorch_lightning.callbacks.stochastic_weight_avg import StochasticWeightAveraging\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "early_stop = EarlyStopping(\n",
    "    monitor = 'validation_bleu',\n",
    "    patience = 2,\n",
    "    mode = 'max',\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='validation_bleu',  # Используйте вашу метрику здесь\n",
    "    mode='max',  # Лучшее значение - это максимальное значение метрики\n",
    "    save_top_k=1,  # Сохраняем только одну лучшую модель\n",
    "    filename='best',  # Имя файла для сохранения\n",
    "    dirpath='models',  # Путь к директории для сохранения\n",
    "    verbose=True,  # Выводить сообщения о процессе сохранения \n",
    ")\n",
    "\n",
    "logger = CometLogger(\n",
    "  api_key=\"cczFfSGpZlad44ZgJP96GKW5U\",\n",
    "  project_name=\"DeepSeek_1,3b\"\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "trainer = Trainer(gradient_clip_algorithm=\"norm\",log_every_n_steps = 150,gradient_clip_val = 1,min_epochs = 10,enable_checkpointing = False,accumulate_grad_batches=16,callbacks = [early_stop,lr_monitor],logger = logger)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e715295a-4208-4b22-84b0-073a23e4826c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda72e3-85be-4ea7-88ff-34561abb192f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fcfccb-2306-4dcb-9b70-5a79f354bebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845e7ed-48c6-4944-83fd-fb95d24bfb56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_test = ['train','test','validation']\n",
    "pred = []\n",
    "truth = []\n",
    "for elem in to_test:\n",
    "  test_final_dataloader = DataLoader(dataset[elem], batch_size=1)\n",
    "  example = next(iter(test_final_dataloader))\n",
    "\n",
    "  # Получите предсказание модели\n",
    "  with torch.no_grad():\n",
    "      output = model.forward(**example)\n",
    "  # Получите идентификаторы предсказанных токенов\n",
    "  pred_ids = torch.argmax(output.logits, dim=-1)\n",
    "  ground_truth = example['labels']\n",
    "  pred_ids = [[i for i in j if i!=0] for j in pred_ids]\n",
    "  # Преобразуйте идентификаторы токенов в текст\n",
    "  pred_text = [tokenizer.decode(ids, skip_special_tokens=True) for ids in pred_ids if ids!=32014 and ids!=0]\n",
    "  ground_truth_list =  torch.tensor([value for value in ground_truth.squeeze().tolist() if value != 32014 and value!=-100 and value!=0])\n",
    "  decoded_text = tokenizer.decode(ground_truth_list)\n",
    "  # Выведите результат\n",
    "  pred.append(''.join(pred_text))\n",
    "  truth.append(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff11e29-1c3c-497d-bfca-dc955baa2096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "  print('Generated:' ,(''.join([i for i in pred[i] if i!='\\n'])),len(pred[i]))\n",
    "  print('Correct: ',truth[i])\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9bb9e5-56fe-46b5-ba9c-5be397d59e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e02b63c-5366-441b-84b3-dd1d1f811d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Открываем файл для записи\n",
    "with open('output.txt', 'w') as file:\n",
    "    for i in range(3):\n",
    "        # Формируем строку для записи\n",
    "        generated_text = ''.join([i for i in pred[i] if i!='\\n'])\n",
    "        generated_length = len(pred[i])\n",
    "        correct_text = truth[i]\n",
    "        output_string = f'Generated: {generated_text}, Length: {generated_length}\\nCorrect: {correct_text}\\n\\n'\n",
    "        \n",
    "        # Записываем строку в файл\n",
    "        file.write(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0947e1-38be-4ab8-b03d-3d225b616b03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/jupyter/datasphere/project/model.ckpt'\n",
    "\n",
    "# Создание экземпляра модели\n",
    "model_best = DeepSeek()\n",
    "\n",
    "# Загрузка весов модели из checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcea08d-0a2d-4d8d-a5b7-fcf35547b09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cc215-02b2-4bc0-9b7a-a99a0a1fe8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196192f2-8363-474b-9580-53fbab4c40ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_test = ['train','test','validation']\n",
    "pred = []\n",
    "truth = []\n",
    "for elem in to_test:\n",
    "  test_final_dataloader = DataLoader(dataset[elem], batch_size=1)\n",
    "  example = next(iter(test_final_dataloader))\n",
    "\n",
    "  # Получите предсказание модели\n",
    "  with torch.no_grad():\n",
    "      output = model.forward(**example)\n",
    "  # Получите идентификаторы предсказанных токенов\n",
    "  pred_ids = torch.argmax(output.logits, dim=-1)\n",
    "  ground_truth = example['labels']\n",
    "  pred_ids = [[i for i in j if i!=0] for j in pred_ids]\n",
    "  # Преобразуйте идентификаторы токенов в текст\n",
    "  pred_text = [tokenizer.decode(ids, skip_special_tokens=True) for ids in pred_ids if ids!=32014 and ids!=0]\n",
    "  ground_truth_list =  torch.tensor([value for value in ground_truth.squeeze().tolist() if value != 32014 and value!=-100 and value!=0])\n",
    "  decoded_text = tokenizer.decode(ground_truth_list)\n",
    "  # Выведите результат\n",
    "  pred.append(''.join(pred_text))\n",
    "  truth.append(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5245a06-8e8f-47c7-976f-e706a92dc132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "  print('Generated:' ,(''.join([i for i in pred[i] if i!='\\n'])),len(pred[i]))\n",
    "  print('Correct: ',truth[i])\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab7cc9-6b55-4be4-9b70-045d7f0d5f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('output_best.txt', 'w') as file:\n",
    "    for i in range(3):\n",
    "        # Формируем строку для записи\n",
    "        generated_text = ''.join([i for i in pred[i] if i!='\\n'])\n",
    "        generated_length = len(pred[i])\n",
    "        correct_text = truth[i]\n",
    "        output_string = f'Generated: {generated_text}, Length: {generated_length}\\nCorrect: {correct_text}\\n\\n'\n",
    "        \n",
    "        # Записываем строку в файл\n",
    "        file.write(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d3564-216d-4241-aa15-238377e20a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"php\", split=\"train[:2600]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c60bf-0ccb-4e6b-ad59-f9e1fbde0b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "length = [len(i) for i in info_dataset['code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a5e24-07a4-4cc4-91b3-11197c0049c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "length.index(max(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7ee62-196c-4013-9a61-b15879ed82fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_dataset['code'][2086]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f201be-825e-4153-b28f-90434143087e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Сортировка массива по возрастанию\n",
    "sorted_length = sorted(length)\n",
    "\n",
    "# Медианное значение\n",
    "median = np.median(sorted_length)\n",
    "\n",
    "# Минимальное значение\n",
    "min_value = np.min(sorted_length)\n",
    "\n",
    "# Максимальное значение\n",
    "max_value = np.max(sorted_length)\n",
    "\n",
    "# Среднее значение\n",
    "mean = np.mean(sorted_length)\n",
    "\n",
    "# Стандартное отклонение\n",
    "std_dev = np.std(sorted_length)\n",
    "\n",
    "# Диапазон значений\n",
    "range_value = max_value - min_value\n",
    "\n",
    "# Количество элементов в массиве\n",
    "num_elements = len(sorted_length)\n",
    "\n",
    "# Квартили\n",
    "Q1 = np.percentile(sorted_length, 25)\n",
    "Q2 = np.percentile(sorted_length, 50)\n",
    "Q3 = np.percentile(sorted_length, 75)\n",
    "\n",
    "print(\"Медианное значение:\", median)\n",
    "print(\"Минимальное значение:\", min_value)\n",
    "print(\"Максимальное значение:\", max_value)\n",
    "print(\"Среднее значение:\", mean)\n",
    "print(\"Стандартное отклонение:\", std_dev)\n",
    "print(\"Диапазон значений:\", range_value)\n",
    "print(\"Количество элементов в массиве:\", num_elements)\n",
    "print(\"Первый квартиль (Q1):\", Q1)\n",
    "print(\"Второй квартиль (Q2):\", Q2)\n",
    "print(\"Третий квартиль (Q3):\", Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cc677-08b2-4a6f-91d0-1fc87548f202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
